# -*- coding: utf-8 -*-
"""Hate Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10ojujOdI5yskJPeqt28EgcOxLG1qsSi3
"""

import gradio as gr
import torch
import tensorflow as tf
import numpy as np
import re
import h5py
import pandas as pd
import html
from transformers import BertModel, BertTokenizer
import torch.nn as nn
from tensorflow.keras.models import model_from_json, Sequential
from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dropout, Dense
from tensorflow.keras.preprocessing.text import Tokenizer as KerasTokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from nltk.stem import WordNetLemmatizer
from lime.lime_text import LimeTextExplainer
import shap
import matplotlib.pyplot as plt

# Device configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load models and tokenizers
bert_model = BertModel.from_pretrained('bert-base-uncased')
bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

class HateSpeechClassifier(nn.Module):
    def __init__(self):
        super(HateSpeechClassifier, self).__init__()
        self.bert = bert_model
        self.dropout = nn.Dropout(0.1)
        self.linear1 = nn.Linear(768, 256)
        self.linear2 = nn.Linear(256, 2)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        pooled_output = outputs[0][:, 0, :]
        x = self.dropout(pooled_output)
        x = torch.relu(self.linear1(x))
        x = self.linear2(x)
        return x

bert_classifier = HateSpeechClassifier()
bert_classifier.load_state_dict(torch.load('hate_speech_model.pt', map_location=device))
bert_classifier.to(device)
bert_classifier.eval()

def load_custom_model(h5_path):
    with h5py.File(h5_path, 'r') as f:
        model_config = f.attrs.get('model_config')
        if model_config:
            model_config = model_config.replace('"time_major": false,', '')
            custom_objects = {
                'Sequential': Sequential,
                'Embedding': Embedding,
                'Conv1D': Conv1D,
                'MaxPooling1D': MaxPooling1D,
                'Bidirectional': Bidirectional,
                'LSTM': LSTM,
                'Dropout': Dropout,
                'Dense': Dense
            }
            model = model_from_json(model_config, custom_objects=custom_objects)
            model.load_weights(h5_path)
            return model
        else:
            raise ValueError('No model configuration found in the .h5 file.')

hsc_model = load_custom_model('hsc_model.h5')

# Preprocessing
lemmatizer = WordNetLemmatizer()

def clean_text(text):
    text = re.sub(r"@[A-Za-z0-9_-]+", 'USR', text)
    text = re.sub(r"http\S+", 'URL', text)
    text = re.sub(r'[^\w\s]', '', text)
    return text.lower()

def preprocess_text(text):
    text = ' '.join(lemmatizer.lemmatize(word) for word in text.split())
    text = re.sub(r'@\w+', '', text)
    text = re.sub(r'http\S+', '', text)
    text = re.sub(r'[^A-Za-z0-9\s]', '', text)
    text = text.lower().strip()
    return text

# Load and fit tokenizer for custom model
hsc = pd.read_csv("Classification.csv")
keras_tokenizer = KerasTokenizer(num_words=10000)
keras_tokenizer.fit_on_texts(hsc['tweet'].astype(str))

# LIME explanation
def model_predict_proba(texts):
    sequences = keras_tokenizer.texts_to_sequences(texts)
    padded_sequences = pad_sequences(sequences, maxlen=150)
    return hsc_model.predict(padded_sequences)

def get_lime_explanation(text, num_features=10):
    class_names = ["Anti-State", "Anti-Religion", "Offensive", "Sexism", "Racism"]
    explainer = LimeTextExplainer(class_names=class_names)
    exp = explainer.explain_instance(text, model_predict_proba, num_features=num_features, top_labels=1)
    return exp

# SHAP explanation
def get_shap_explanation(text, num_features=10):
    sequence = keras_tokenizer.texts_to_sequences([text])
    padded_sequence = pad_sequences(sequence, maxlen=150).astype(np.float32)

    background = hsc['tweet'].iloc[:100].apply(preprocess_text).values
    background_sequences = keras_tokenizer.texts_to_sequences(background)
    background_data = pad_sequences(background_sequences, maxlen=150).astype(np.float32)

    explainer = shap.DeepExplainer(hsc_model, background_data)
    shap_values = explainer.shap_values(padded_sequence)

    words = text.split()
    word_importances = []

    for class_idx, class_shap_values in enumerate(shap_values):
        for i, word in enumerate(words):
            importance = np.sum(np.abs(class_shap_values[0][i]))
            word_importances.append((word, importance, class_idx))

    word_importances.sort(key=lambda x: abs(x[1]), reverse=True)
    return word_importances[:num_features]

# Main analysis function
def integrated_hate_speech_analysis(text):
    preprocessed_text = preprocess_text(text)
    sequence = keras_tokenizer.texts_to_sequences([preprocessed_text])
    padded_sequence = pad_sequences(sequence, maxlen=150)
    
    # Classify hate speech
    probabilities = hsc_model.predict(padded_sequence)[0]
    category_index = np.argmax(probabilities)
    labels = ["Anti-State", "Anti-Religion", "Offensive", "Sexism", "Racism"]
    category_label = labels[category_index]
    
    # Get LIME explanation
    lime_exp = get_lime_explanation(preprocessed_text)
    lime_explanation = lime_exp.as_list(label=category_index)
    
    # Get SHAP explanation
    shap_explanation = get_shap_explanation(preprocessed_text)
    
    # Create result HTML
    result = f"<h2>Hate Speech Analysis</h2>"
    result += f"<p>Category: {category_label}</p>"
    result += f"<p>Confidence: {probabilities[category_index]:.2%}</p>"
    
    result += "<h3>LIME Explanation:</h3><ul>"
    for feature, importance in lime_explanation:
        color = "red" if importance < 0 else "green"
        result += f'<li><span style="color: {color};">{html.escape(feature)}</span>: {importance:.4f}</li>'
    result += "</ul>"
    
    result += "<h3>SHAP Explanation:</h3><ul>"
    for word, importance, _ in shap_explanation:
        color = "red" if importance < 0 else "green"
        result += f'<li><span style="color: {color};">{html.escape(word)}</span>: {importance:.4f}</li>'
    result += "</ul>"
    
    # Create LIME plot
    plt.figure(figsize=(10, 6))
    lime_exp.as_pyplot_figure(label=category_index)
    plt.title(f"LIME Explanation for '{category_label}'")
    plt.tight_layout()
    plt.savefig('lime_explanation.png')
    plt.close()
    
    # Create SHAP plot
    plt.figure(figsize=(10, 6))
    shap.summary_plot(shap_values, padded_sequence, feature_names=preprocessed_text.split(), plot_type="bar", show=False)
    plt.title(f"SHAP Explanation for '{category_label}'")
    plt.tight_layout()
    plt.savefig('shap_explanation.png')
    plt.close()
    
    return result, 'lime_explanation.png', 'shap_explanation.png'

# Gradio interface
iface = gr.Interface(
    fn=integrated_hate_speech_analysis,
    inputs=gr.Textbox(label="Enter text for analysis", lines=5),
    outputs=[
        gr.HTML(label="Analysis Result"),
        gr.Image(label="LIME Visualization"),
        gr.Image(label="SHAP Visualization")
    ],
    title="Integrated Hate Speech Analysis with LIME and SHAP",
    description="Enter a text to detect and classify hate speech, with explanations using LIME and SHAP.",
    theme="huggingface",
    css="body { background-color: #f0f0f0; } .gradio-container { box-shadow: 0 0 10px rgba(0,0,0,0.1); border-radius: 10px; }"
)

iface.launch()